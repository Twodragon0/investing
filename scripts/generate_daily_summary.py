#!/usr/bin/env python3
"""Generate a daily news summary post by reading today's collected posts.

Reads all posts generated for the current date and creates a comprehensive
summary post (pinned, market-analysis category) with priority-based structure:
1. Urgent alerts (P0) - crashes, hacks, executive orders
2. Market overview
3. Indicator dashboard
4. Political watch - politician trades/policy highlights
5. Important news (P1) - regulation, ETF, earnings
6. Category summaries
7. Notable news (P2)
8. Report links
"""

import sys
import os
import re
import glob
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Tuple

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from common.config import setup_logging
from common.post_generator import POSTS_DIR
from common.summarizer import ThemeSummarizer

logger = setup_logging("generate_daily_summary")


def read_post_content(filepath: str) -> Dict[str, Any]:
    """Read a Jekyll post and parse frontmatter + content."""
    with open(filepath, "r", encoding="utf-8") as f:
        text = f.read()

    # Parse frontmatter
    parts = text.split("---", 2)
    if len(parts) < 3:
        return {"frontmatter": {}, "content": text, "filepath": filepath}

    frontmatter_text = parts[1].strip()
    content = parts[2].strip()

    frontmatter = {}
    for line in frontmatter_text.split("\n"):
        if ":" in line:
            key, _, value = line.partition(":")
            key = key.strip()
            value = value.strip().strip('"').strip("'")
            frontmatter[key] = value

    return {
        "frontmatter": frontmatter,
        "content": content,
        "filepath": filepath,
    }


def extract_section(content: str, heading: str) -> str:
    """Extract content under a specific ## heading."""
    pattern = rf"## {re.escape(heading)}\s*\n(.*?)(?=\n## |\Z)"
    match = re.search(pattern, content, re.DOTALL)
    return match.group(1).strip() if match else ""


def extract_bullet_points(content: str, heading: str, max_items: int = 5) -> List[str]:
    """Extract bullet points from a section."""
    section = extract_section(content, heading)
    if not section:
        return []

    bullets = []
    for line in section.split("\n"):
        line = line.strip()
        if line.startswith("- "):
            bullets.append(line)
            if len(bullets) >= max_items:
                break
    return bullets


def extract_table_rows(content: str, heading: str, max_rows: int = 10) -> List[str]:
    """Extract table rows (excluding header) from a section."""
    section = extract_section(content, heading)
    if not section:
        return []

    rows = []
    header_passed = 0
    for line in section.split("\n"):
        line = line.strip()
        if line.startswith("|"):
            header_passed += 1
            if header_passed > 2:  # Skip header + separator
                rows.append(line)
                if len(rows) >= max_rows:
                    break
    return rows


def count_news_items(content: str) -> int:
    """Try to extract total news count from content."""
    patterns = [
        r"(\d+)ê±´ì˜ ë‰´ìŠ¤",
        r"ì´ ë‰´ìŠ¤ ê±´ìˆ˜\*?\*?:\s*(\d+)ê±´",
        r"ì´ ìˆ˜ì§‘ ê±´ìˆ˜\*?\*?:\s*(\d+)ê±´",
        r"ì´ (\d+)ê±´",
        r"(\d+)ê±´ì´ ìˆ˜ì§‘",
    ]
    for pattern in patterns:
        match = re.search(pattern, content)
        if match:
            return int(match.group(1))
    return 0


def _extract_highlights(content: str) -> List[str]:
    """Extract highlight info from post opening and alert-info HTML."""
    highlights = []
    # Try opening paragraph (first non-empty line after frontmatter)
    for line in content.split("\n")[:5]:
        line = line.strip()
        if line.startswith("**") and "ê±´" in line:
            highlights.append(f"- {line}")
            break
    # Try old-style sections as fallback
    for section in ["ì˜¤ëŠ˜ì˜ í•µì‹¬", "í•µì‹¬ ìš”ì•½"]:
        bullets = extract_bullet_points(content, section)
        if bullets:
            highlights.extend(bullets)
            break
    # Try alert-info content
    match = re.search(
        r'class="alert-box alert-info"[^>]*>.*?<strong>(.*?)</strong>', content
    )
    if match and not highlights:
        highlights.append(f"- {match.group(1)}")
    return highlights


def summarize_crypto_post(post: Dict[str, Any]) -> Dict[str, Any]:
    """Extract key info from crypto news post."""
    content = post["content"]
    count = count_news_items(content)
    highlights = _extract_highlights(content)

    # Extract themes from HTML progress bars or ASCII chart
    themes = []
    for match in re.finditer(
        r'class="theme-label">.\s*(\S+)</span>.*?(\d+)ê±´', content
    ):
        themes.append((match.group(1), int(match.group(2))))
    if not themes:
        dist_section = extract_section(content, "ì´ìŠˆ ë¶„í¬ í˜„í™©")
        if dist_section:
            for line in dist_section.split("\n"):
                m = re.match(r"(\S+)\s+[â–ˆâ–‘]+\s+\d+%\s+\((\d+)ê±´\)", line.strip())
                if m:
                    themes.append((m.group(1), int(m.group(2))))

    return {
        "type": "crypto",
        "title": post["frontmatter"].get("title", ""),
        "count": count,
        "highlights": highlights,
        "key_summary": highlights,
        "themes": themes,
        "content": content,
    }


def summarize_stock_post(post: Dict[str, Any]) -> Dict[str, Any]:
    """Extract key info from stock news post."""
    content = post["content"]
    count = count_news_items(content)
    highlights = _extract_highlights(content)

    market_data = []
    for line in content.split("\n")[:5]:
        if "KOSPI" in line or "KOSDAQ" in line or "USD/KRW" in line:
            market_data.append(line.strip())

    return {
        "type": "stock",
        "title": post["frontmatter"].get("title", ""),
        "count": count,
        "highlights": highlights,
        "key_summary": highlights,
        "market_data": market_data,
        "content": content,
    }


def summarize_security_post(post: Dict[str, Any]) -> Dict[str, Any]:
    """Extract key info from security post."""
    content = post["content"]
    count = count_news_items(content)
    key_summary = _extract_highlights(content)
    incidents = extract_table_rows(content, "ë³´ì•ˆ ì‚¬ê³  í˜„í™©", 5)

    return {
        "type": "security",
        "title": post["frontmatter"].get("title", ""),
        "count": count,
        "key_summary": key_summary,
        "incidents": incidents,
        "content": content,
    }


def summarize_regulatory_post(post: Dict[str, Any]) -> Dict[str, Any]:
    """Extract key info from regulatory post."""
    content = post["content"]
    count = count_news_items(content)
    key_summary = _extract_highlights(content)

    return {
        "type": "regulatory",
        "title": post["frontmatter"].get("title", ""),
        "count": count,
        "key_summary": key_summary,
        "content": content,
    }


def summarize_social_post(post: Dict[str, Any]) -> Dict[str, Any]:
    """Extract key info from social media post."""
    content = post["content"]
    count = count_news_items(content)
    highlights = _extract_highlights(content)

    return {
        "type": "social",
        "title": post["frontmatter"].get("title", ""),
        "count": count,
        "highlights": highlights,
        "key_summary": highlights,
        "content": content,
    }


def summarize_market_post(post: Dict[str, Any]) -> Dict[str, Any]:
    """Extract key info from market summary post."""
    content = post["content"]
    highlights = extract_bullet_points(content, "ì˜¤ëŠ˜ì˜ í•µì‹¬")
    exec_summary = extract_bullet_points(content, "í•œëˆˆì— ë³´ê¸°")

    # Extract indicator data
    indicator_rows = extract_table_rows(content, "ë§¤í¬ë¡œ ê²½ì œ ì§€í‘œ", 10)
    yield_section = extract_section(content, "êµ­ì±„ ìˆ˜ìµë¥  ìŠ¤í”„ë ˆë“œ (2Y-10Y)")
    sector_section = extract_section(content, "S&P 500 ì„¹í„° í¼í¬ë¨¼ìŠ¤")

    return {
        "type": "market",
        "title": post["frontmatter"].get("title", ""),
        "highlights": highlights,
        "exec_summary": exec_summary,
        "indicator_rows": indicator_rows,
        "yield_section": yield_section,
        "sector_section": sector_section,
        "content": content,
    }


def summarize_political_post(post: Dict[str, Any]) -> Dict[str, Any]:
    """Extract key info from political trades post."""
    content = post["content"]
    count = count_news_items(content)
    key_summary = extract_bullet_points(content, "í•µì‹¬ ìš”ì•½")
    highlights = extract_bullet_points(content, "ì •ì±… ì˜í–¥ ë¶„ì„", 3)

    return {
        "type": "political",
        "title": post["frontmatter"].get("title", ""),
        "count": count,
        "key_summary": key_summary,
        "highlights": highlights,
        "content": content,
    }


def get_post_url(filepath: str, today: str, category: str = "") -> str:
    """Generate relative URL for a post following Jekyll permalink structure."""
    filename = os.path.basename(filepath)
    slug = filename.replace(f"{today}-", "").replace(".md", "")
    date_path = today.replace("-", "/")
    if category:
        return f"/{category}/{date_path}/{slug}/"
    return f"/{date_path}/{slug}/"


def _collect_all_news_items(summaries: List[Optional[Dict]]) -> List[Dict[str, Any]]:
    """Collect all news item titles+descriptions from post contents for priority classification."""
    items = []
    seen_titles = set()
    for s in summaries:
        if not s or not s.get("content"):
            continue
        content = s["content"]
        in_card_section = False
        for line in content.split("\n"):
            line = line.strip()
            # Detect card section headers (### ğŸŸ  ë¹„íŠ¸ì½”ì¸, etc.)
            if line.startswith("### ") and "ê±´)" in line:
                in_card_section = True
                continue
            # Extract from card format: **1. [Title](link)**
            if in_card_section and line.startswith("**") and "[" in line:
                match = re.search(r"\[([^\]]+)\]\(([^)]+)\)", line)
                if match:
                    title = match.group(1)
                    link = match.group(2)
                    # Deduplicate by normalized title
                    norm = re.sub(r"[^a-zê°€-í£0-9]", "", title.lower())
                    if norm not in seen_titles:
                        seen_titles.add(norm)
                        items.append(
                            {
                                "title": f"[{title}]({link})",
                                "description": title,
                                "source": s.get("type", ""),
                            }
                        )
            # Also extract description lines right after card items
            if (
                in_card_section
                and not line.startswith("**")
                and not line.startswith(">")
                and not line.startswith("`")
                and not line.startswith("#")
                and not line.startswith("|")
                and line
                and not line.startswith("---")
            ):
                # This might be a description line, skip it for item collection
                pass
            # Stop card parsing at next major section
            if line.startswith("## ") and in_card_section:
                in_card_section = False
    return items


def _cross_asset_topics() -> Dict[str, List[str]]:
    return {
        "ê¸ˆë¦¬/ìœ ë™ì„±": ["ê¸ˆë¦¬", "ì—°ì¤€", "fed", "fomc", "ìœ ë™ì„±", "êµ­ì±„", "yield"],
        "í™˜ìœ¨/ë‹¬ëŸ¬": ["í™˜ìœ¨", "usd/krw", "ë‹¬ëŸ¬", "dxy"],
        "ì •ì±…/ê·œì œ": [
            "ê·œì œ",
            "sec",
            "etf",
            "ë²•ì•ˆ",
            "ì •ì±…",
            "í–‰ì •ëª…ë ¹",
            "tariff",
            "ê´€ì„¸",
        ],
        "ë¦¬ìŠ¤í¬ ì´ë²¤íŠ¸": ["í•´í‚¹", "exploit", "íŒŒì‚°", "ì²­ì‚°", "liquidation", "ë³´ì•ˆì‚¬ê³ "],
        "ìˆ˜ê¸‰/ì‹¬ë¦¬": ["ê³ ë˜", "whale", "ìˆ˜ê¸‰", "ê³µí¬", "íƒìš•", "sentiment", "social"],
        "ì‹¤ì /ì§€í‘œ": ["ì‹¤ì ", "cpi", "pce", "ê³ ìš©", "ë§¤ì¶œ", "earnings"],
    }


def _topic_hits(summary: Optional[Dict[str, Any]]) -> Dict[str, int]:
    if not summary:
        return {}
    text = "\n".join(
        [
            summary.get("title", ""),
            summary.get("content", ""),
            " ".join(summary.get("highlights", []) or []),
            " ".join(summary.get("key_summary", []) or []),
        ]
    ).lower()
    hits: Dict[str, int] = {}
    for topic, keywords in _cross_asset_topics().items():
        score = 0
        for kw in keywords:
            score += text.count(kw.lower())
        hits[topic] = score
    return hits


def _relation_rows(
    summaries: Dict[str, Optional[Dict[str, Any]]],
) -> List[Tuple[str, str, int, str]]:
    rows: List[Tuple[str, str, int, str]] = []
    pairs = [
        ("ì•”í˜¸í™”í", "ì£¼ì‹", "crypto", "stock"),
        ("ì•”í˜¸í™”í", "ì •ì¹˜ì¸ ê±°ë˜", "crypto", "political"),
        ("ì£¼ì‹", "ì •ì¹˜ì¸ ê±°ë˜", "stock", "political"),
        ("ì•”í˜¸í™”í", "ê·œì œ", "crypto", "regulatory"),
        ("ì£¼ì‹", "ê·œì œ", "stock", "regulatory"),
        ("ì•”í˜¸í™”í", "ì†Œì…œ", "crypto", "social"),
    ]
    topic_keys = list(_cross_asset_topics().keys())
    hit_maps = {k: _topic_hits(v) for k, v in summaries.items()}

    for left_name, right_name, left_key, right_key in pairs:
        left = hit_maps.get(left_key, {})
        right = hit_maps.get(right_key, {})
        if not left or not right:
            continue
        shared_topics: List[Tuple[str, int]] = []
        for t in topic_keys:
            if left.get(t, 0) > 0 and right.get(t, 0) > 0:
                shared_topics.append((t, min(left[t], right[t])))
        shared_topics.sort(key=lambda x: x[1], reverse=True)
        if not shared_topics:
            rows.append((left_name, right_name, 0, "ë‚®ìŒ"))
            continue
        score = sum(v for _, v in shared_topics[:3])
        top_topics = ", ".join(t for t, _ in shared_topics[:2])
        if score >= 8:
            level = "ë†’ìŒ"
        elif score >= 4:
            level = "ì¤‘ê°„"
        else:
            level = "ë‚®ìŒ"
        rows.append((left_name, right_name, score, f"{level} ({top_topics})"))
    return rows


def _coverage_warnings(summaries: Dict[str, Optional[Dict[str, Any]]]) -> List[str]:
    warnings = []
    if not summaries.get("crypto"):
        warnings.append(
            "- ì•”í˜¸í™”í ì¼ì¼ ë¦¬í¬íŠ¸ê°€ ì—†ì–´ ì½”ì¸-ì£¼ì‹ ì—°ê³„ ë¶„ì„ ì •ë°€ë„ê°€ ë‚®ìŠµë‹ˆë‹¤."
        )
    if not summaries.get("stock"):
        warnings.append("- ì£¼ì‹ ì¼ì¼ ë¦¬í¬íŠ¸ê°€ ì—†ì–´ êµì°¨ìì‚° ìˆ˜ê¸‰ ë¹„êµê°€ ì œí•œë©ë‹ˆë‹¤.")
    if not summaries.get("market"):
        warnings.append(
            "- ì‹œì¥ ì¢…í•© ë¦¬í¬íŠ¸ê°€ ì—†ì–´ ë§¤í¬ë¡œ(ê¸ˆë¦¬/í™˜ìœ¨) ì—°ê²° í•´ì„ì´ ì œí•œë©ë‹ˆë‹¤."
        )
    if not summaries.get("political") and not summaries.get("regulatory"):
        warnings.append(
            "- ì •ì±…/ê·œì œ ë°ì´í„°ê°€ ë¶€ì¡±í•´ ì´ë²¤íŠ¸ ê¸°ë°˜ ë¦¬ìŠ¤í¬ ì ê²€ì´ ì•½í•©ë‹ˆë‹¤."
        )
    return warnings


def _strip_markdown_link(text: str) -> str:
    text = re.sub(r"\[([^\]]+)\]\([^\)]+\)", r"\1", text)
    text = text.replace("**", "")
    return text.strip()


def _to_theme_payload(
    summaries: Dict[str, Optional[Dict[str, Any]]],
) -> List[Dict[str, Any]]:
    payload = []
    topic_def = _cross_asset_topics()
    hit_totals = {k: 0 for k in topic_def.keys()}
    for summary in summaries.values():
        hits = _topic_hits(summary)
        for k, v in hits.items():
            hit_totals[k] += v

    emojis = {
        "ê¸ˆë¦¬/ìœ ë™ì„±": "ğŸ¦",
        "í™˜ìœ¨/ë‹¬ëŸ¬": "ğŸ’µ",
        "ì •ì±…/ê·œì œ": "ğŸ“œ",
        "ë¦¬ìŠ¤í¬ ì´ë²¤íŠ¸": "ğŸš¨",
        "ìˆ˜ê¸‰/ì‹¬ë¦¬": "ğŸ§­",
        "ì‹¤ì /ì§€í‘œ": "ğŸ“Š",
    }

    for topic, score in sorted(hit_totals.items(), key=lambda x: x[1], reverse=True):
        if score <= 0:
            continue
        payload.append(
            {
                "name": topic,
                "emoji": emojis.get(topic, "â€¢"),
                "count": score,
                "keywords": topic_def.get(topic, [])[:4],
            }
        )
    return payload[:5]


def _render_generated_image(filename: str, alt: str) -> Optional[str]:
    image_path = os.path.join(
        POSTS_DIR, "..", "assets", "images", "generated", filename
    )
    if not os.path.exists(image_path):
        return None
    return f"![{alt}]({{{{ '/assets/images/generated/{filename}' | relative_url }}}})"


def _build_snapshot_table(
    crypto_summary: Optional[Dict[str, Any]],
    stock_summary: Optional[Dict[str, Any]],
    regulatory_summary: Optional[Dict[str, Any]],
    social_summary: Optional[Dict[str, Any]],
    political_summary: Optional[Dict[str, Any]],
) -> List[str]:
    rows = [
        "| ì˜ì—­ | ìˆ˜ì§‘ ê±´ìˆ˜ | í•µì‹¬ ì‹ í˜¸ |",
        "|------|---------:|-----------|",
    ]

    def top_signal(summary: Optional[Dict[str, Any]]) -> str:
        if not summary:
            return "ë°ì´í„° ì—†ìŒ"
        if summary.get("themes"):
            name, cnt = summary["themes"][0]
            return f"{name} {cnt}ê±´"
        hl = summary.get("highlights") or summary.get("key_summary") or []
        if hl:
            return _strip_markdown_link(hl[0])[:80]
        return "ì‹ í˜¸ ì¶”ì¶œ ì‹¤íŒ¨"

    dataset = [
        ("ì•”í˜¸í™”í", crypto_summary),
        ("ì£¼ì‹", stock_summary),
        ("ê·œì œ", regulatory_summary),
        ("ì†Œì…œ", social_summary),
        ("ì •ì¹˜ì¸ ê±°ë˜", political_summary),
    ]
    for name, summary in dataset:
        count = summary.get("count", 0) if summary else 0
        rows.append(f"| {name} | {count} | {top_signal(summary)} |")
    return rows


def main():
    """Generate daily news summary with priority-based structure."""
    logger.info("=== Generating daily news summary ===")

    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # Find all posts for today
    pattern = os.path.join(POSTS_DIR, f"{today}-*.md")
    today_posts = sorted(glob.glob(pattern))

    if not today_posts:
        logger.warning("No posts found for today (%s), skipping summary", today)
        return

    logger.info("Found %d posts for %s", len(today_posts), today)

    # Read and categorize posts
    crypto_summary = None
    stock_summary = None
    security_summary = None
    regulatory_summary = None
    social_summary = None
    market_summary = None
    political_summary = None

    post_links = []

    for filepath in today_posts:
        filename = os.path.basename(filepath)
        if "daily-news-summary" in filename or "test-post" in filename:
            continue

        post = read_post_content(filepath)
        slug = filename.replace(f"{today}-", "").replace(".md", "")

        if "crypto-news-digest" in slug:
            crypto_summary = summarize_crypto_post(post)
            crypto_summary["url"] = get_post_url(filepath, today, "crypto-news")
            post_links.append(
                ("ì•”í˜¸í™”í ë‰´ìŠ¤", crypto_summary["count"], crypto_summary["url"])
            )
        elif "stock-news-digest" in slug:
            stock_summary = summarize_stock_post(post)
            stock_summary["url"] = get_post_url(filepath, today, "stock-news")
            post_links.append(
                ("ì£¼ì‹ ì‹œì¥ ë‰´ìŠ¤", stock_summary["count"], stock_summary["url"])
            )
        elif "security-report" in slug:
            security_summary = summarize_security_post(post)
            security_summary["url"] = get_post_url(filepath, today, "security-alerts")
            post_links.append(
                ("ë³´ì•ˆ ë¦¬í¬íŠ¸", security_summary["count"], security_summary["url"])
            )
        elif "regulatory-report" in slug:
            regulatory_summary = summarize_regulatory_post(post)
            regulatory_summary["url"] = get_post_url(filepath, today, "regulatory-news")
            post_links.append(
                ("ê·œì œ ë™í–¥", regulatory_summary["count"], regulatory_summary["url"])
            )
        elif "social-media-digest" in slug:
            social_summary = summarize_social_post(post)
            social_summary["url"] = get_post_url(filepath, today, "crypto-news")
            post_links.append(
                ("ì†Œì…œ ë¯¸ë””ì–´", social_summary["count"], social_summary["url"])
            )
        elif "political-trades-report" in slug:
            political_summary = summarize_political_post(post)
            political_summary["url"] = get_post_url(filepath, today, "political-trades")
            post_links.append(
                ("ì •ì¹˜ì¸ ê±°ë˜", political_summary["count"], political_summary["url"])
            )
        elif "market-report" in slug:
            market_summary = summarize_market_post(post)
            market_summary["url"] = get_post_url(filepath, today, "market-analysis")
            post_links.append(("ì‹œì¥ ì¢…í•© ë¦¬í¬íŠ¸", 0, market_summary["url"]))

    # Calculate total count
    all_summaries = [
        crypto_summary,
        stock_summary,
        security_summary,
        regulatory_summary,
        social_summary,
        political_summary,
    ]
    total_count = sum(s["count"] for s in all_summaries if s and s.get("count"))

    # Priority classification
    all_news_items = _collect_all_news_items(all_summaries)
    priority_items = {"P0": [], "P1": [], "P2": []}
    if all_news_items:
        summarizer = ThemeSummarizer(all_news_items)
        priority_items = summarizer.classify_priority()

    summary_map = {
        "crypto": crypto_summary,
        "stock": stock_summary,
        "market": market_summary,
        "regulatory": regulatory_summary,
        "social": social_summary,
        "political": political_summary,
    }

    theme_payload = _to_theme_payload(summary_map)
    urgent_alerts = [
        _strip_markdown_link(x.get("title", "")) for x in priority_items.get("P0", [])
    ]

    briefing_image = None
    try:
        from common.image_generator import generate_news_briefing_card

        briefing_image = generate_news_briefing_card(
            themes=theme_payload,
            date_str=today,
            category="Multi-Asset Daily Briefing",
            total_count=total_count,
            urgent_alerts=[x for x in urgent_alerts if x],
            filename=f"news-briefing-daily-{today}.png",
        )
    except Exception as e:
        logger.warning("Failed to generate daily briefing image: %s", e)

    # Build summary content
    content_parts = []

    # Opening with counts
    count_parts = []
    if crypto_summary and crypto_summary["count"]:
        count_parts.append(f"ì•”í˜¸í™”í {crypto_summary['count']}ê±´")
    if stock_summary and stock_summary["count"]:
        count_parts.append(f"ì£¼ì‹ {stock_summary['count']}ê±´")
    if security_summary and security_summary["count"]:
        count_parts.append(f"ë³´ì•ˆ {security_summary['count']}ê±´")
    if regulatory_summary and regulatory_summary["count"]:
        count_parts.append(f"ê·œì œ {regulatory_summary['count']}ê±´")
    if social_summary and social_summary["count"]:
        count_parts.append(f"ì†Œì…œ ë¯¸ë””ì–´ {social_summary['count']}ê±´")
    if political_summary and political_summary["count"]:
        count_parts.append(f"ì •ì¹˜ì¸ ê±°ë˜ {political_summary['count']}ê±´")

    counts_str = ", ".join(count_parts) if count_parts else "ë‰´ìŠ¤"
    content_parts.append(f"> {counts_str}ì˜ ë‰´ìŠ¤ë¥¼ ì¢…í•© ë¶„ì„í•œ ì¼ì¼ ìš”ì•½ì…ë‹ˆë‹¤.\n")

    content_parts.append(
        '<div class="alert-box alert-info"><strong>í•œëˆˆì— ë³´ëŠ” ì‹œì¥ ìƒí™©</strong><ul>'
    )
    content_parts.append(f"<li>ì´ ìˆ˜ì§‘: <strong>{total_count}ê±´</strong></li>")
    content_parts.append(
        f"<li>ê¸´ê¸‰ ì•Œë¦¼(P0): <strong>{len(priority_items.get('P0', []))}ê±´</strong></li>"
    )
    content_parts.append(
        f"<li>ì¤‘ìš” ë‰´ìŠ¤(P1): <strong>{len(priority_items.get('P1', []))}ê±´</strong></li>"
    )
    content_parts.append("</ul></div>\n")

    content_parts.append("## ì¢…í•© ëŒ€ì‹œë³´ë“œ\n")
    content_parts.extend(
        _build_snapshot_table(
            crypto_summary,
            stock_summary,
            regulatory_summary,
            social_summary,
            political_summary,
        )
    )
    content_parts.append("")

    if briefing_image:
        content_parts.append(f"![multi-asset-briefing]({briefing_image})\n")
    fallback_briefing = _render_generated_image(
        f"news-briefing-daily-{today}.png", "multi-asset-briefing"
    )
    if not briefing_image and fallback_briefing:
        content_parts.append(fallback_briefing + "\n")

    indicator_img = _render_generated_image(
        f"indicator-dashboard-{today}.png", "indicator-dashboard"
    )
    if indicator_img:
        content_parts.append(indicator_img + "\n")

    heatmap_img = _render_generated_image(
        f"market-heatmap-{today}.png", "market-heatmap"
    )
    if heatmap_img:
        content_parts.append(heatmap_img + "\n")

    # Executive briefing: 3-5 line summary from each category
    briefing_lines = []
    for s in all_summaries:
        if not s or not s.get("highlights"):
            continue
        # Pick the most informative highlight
        for h in s["highlights"][:1]:
            line = h.strip()
            if line.startswith("- "):
                line = line[2:]
            if len(line) > 15:
                briefing_lines.append(f"> - {line}")
    if briefing_lines:
        content_parts.append("## í•µì‹¬ ë¸Œë¦¬í•‘\n")
        content_parts.extend(briefing_lines)
        content_parts.append("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1. URGENT ALERTS (P0)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if priority_items.get("P0"):
        content_parts.append("## ê¸´ê¸‰ ì•Œë¦¼\n")
        content_parts.append("> ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•œ ê¸´ê¸‰ ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n")
        seen_p0 = set()
        for item in priority_items["P0"][:5]:
            title = item.get("title", "")
            norm = re.sub(r"[^a-zê°€-í£0-9]", "", item.get("description", title).lower())
            if norm in seen_p0:
                continue
            seen_p0.add(norm)
            content_parts.append(f"- **{title}**")
        content_parts.append("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2. MARKET OVERVIEW
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if market_summary:
        content_parts.append("## ì‹œì¥ ê°œìš”\n")
        if market_summary.get("highlights"):
            for h in market_summary["highlights"]:
                content_parts.append(h)
        elif market_summary.get("exec_summary"):
            for h in market_summary["exec_summary"]:
                content_parts.append(h)
        content_parts.append("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3. INDICATOR DASHBOARD
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    indicator_parts = []

    # Macro indicators from market report
    if market_summary and market_summary.get("indicator_rows"):
        indicator_parts.append("| ì§€í‘œ | í˜„ì¬ ê°’ | ë³€ë™ |")
        indicator_parts.append("|------|---------|------|")
        for row in market_summary["indicator_rows"]:
            indicator_parts.append(row)

    # Yield spread
    if market_summary and market_summary.get("yield_section"):
        if indicator_parts:
            indicator_parts.append("")
        indicator_parts.append("**êµ­ì±„ ìˆ˜ìµë¥  ìŠ¤í”„ë ˆë“œ:**")
        # Extract just the key info
        for line in market_summary["yield_section"].split("\n"):
            line = line.strip()
            if line.startswith("|") and "ìŠ¤í”„ë ˆë“œ" in line:
                indicator_parts.append(line)
            elif line.startswith(">"):
                indicator_parts.append(line)

    if indicator_parts:
        content_parts.append("## ì§€í‘œ ëŒ€ì‹œë³´ë“œ\n")
        content_parts.extend(indicator_parts)
        content_parts.append("")

    relation_rows = _relation_rows(summary_map)
    coverage_notes = _coverage_warnings(summary_map)

    if relation_rows or coverage_notes:
        content_parts.append("## êµì°¨ìì‚° ì—°ê´€ì„± ì²´í¬\n")
        content_parts.append(
            "> ë‰´ìŠ¤, ì£¼ì‹, ì½”ì¸, ì •ì¹˜/ê·œì œ ì´ë²¤íŠ¸ë¥¼ ì—°ê²°í•´ ë‹¹ì¼ ë¦¬ìŠ¤í¬/ê¸°íšŒ ì‹ í˜¸ë¥¼ ì ê²€í•©ë‹ˆë‹¤.\n"
        )

        if relation_rows:
            content_parts.append("| ë¹„êµ êµ¬ê°„ | ì—°ê´€ ì ìˆ˜ | ì§„ë‹¨ |")
            content_parts.append("|-----------|-----------:|------|")
            for left, right, score, note in relation_rows:
                content_parts.append(f"| {left} â†” {right} | {score} | {note} |")
            content_parts.append("")

        if coverage_notes:
            content_parts.append("**ë°ì´í„° ì»¤ë²„ë¦¬ì§€ ê²½ê³ **")
            content_parts.extend(coverage_notes)
            content_parts.append("")

        content_parts.append("**ìš´ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸**")
        content_parts.append(
            "- ì—°ê´€ ì ìˆ˜ 'ë†’ìŒ' êµ¬ê°„ì€ ì¥ì¤‘ ë³€ë™ì„± í™•ëŒ€ ê°€ëŠ¥ì„±ìœ¼ë¡œ ìš°ì„  ëª¨ë‹ˆí„°ë§"
        )
        content_parts.append(
            "- ì •ì±…/ê·œì œ + ì •ì¹˜ì¸ ê±°ë˜ê°€ ë™ì‹œ ê¸‰ì¦í•˜ë©´ í¬ì§€ì…˜ ê·œëª¨ì™€ ë ˆë²„ë¦¬ì§€ ë³´ìˆ˜ì ìœ¼ë¡œ ì¡°ì •"
        )
        content_parts.append(
            "- ì½”ì¸/ì£¼ì‹ ëª¨ë‘ ìˆ˜ê¸‰Â·ì‹¬ë¦¬ í‚¤ì›Œë“œê°€ ì¦ê°€í•˜ë©´ ë‹¨ê¸° ê³¼ì—´/ê³¼ë§¤ë„ ë°˜ì „ ê°€ëŠ¥ì„± ì ê²€"
        )
        content_parts.append("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4. POLITICAL WATCH
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if political_summary:
        content_parts.append("---\n")
        content_parts.append("## ì •ì¹˜ì¸ ì›Œì¹˜\n")
        if political_summary.get("key_summary"):
            for h in political_summary["key_summary"][:5]:
                content_parts.append(h)
        if political_summary.get("highlights"):
            content_parts.append("")
            for h in political_summary["highlights"][:3]:
                content_parts.append(h)
        content_parts.append(f"\n[ìƒì„¸ ë³´ê¸°]({political_summary.get('url', '#')})\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5. IMPORTANT NEWS (P1)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if priority_items.get("P1"):
        content_parts.append("---\n")
        content_parts.append("## ì¤‘ìš” ë‰´ìŠ¤\n")
        content_parts.append("> ê·œì œ, ETF, ì‹¤ì  ë“± ì£¼ìš” ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n")
        seen_p1 = set()
        for item in priority_items["P1"][:7]:
            title = item.get("title", "")
            norm = re.sub(r"[^a-zê°€-í£0-9]", "", item.get("description", title).lower())
            if norm in seen_p1:
                continue
            seen_p1.add(norm)
            content_parts.append(f"- {title}")
        content_parts.append("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6. CATEGORY SUMMARIES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    content_parts.append("---\n")
    content_parts.append("## ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½\n")

    # Crypto section with description highlights
    if crypto_summary:
        content_parts.append(f"### ì•”í˜¸í™”í ë‰´ìŠ¤ ({crypto_summary['count']}ê±´)\n")
        if crypto_summary.get("themes"):
            themes_str = ", ".join(
                f"**{t[0]}**({t[1]}ê±´)" for t in crypto_summary["themes"][:4]
            )
            content_parts.append(f"ì£¼ìš” í…Œë§ˆ: {themes_str}\n")
        if crypto_summary.get("highlights"):
            for h in crypto_summary["highlights"][:4]:
                content_parts.append(h)
        elif crypto_summary.get("key_summary"):
            for h in crypto_summary["key_summary"][:4]:
                content_parts.append(h)
        content_parts.append(f"\n[ìƒì„¸ ë³´ê¸°]({crypto_summary.get('url', '#')})\n")

    # Stock section with description highlights
    if stock_summary:
        content_parts.append(f"### ì£¼ì‹ ì‹œì¥ ë‰´ìŠ¤ ({stock_summary['count']}ê±´)\n")
        if stock_summary.get("market_data"):
            for md in stock_summary["market_data"][:3]:
                content_parts.append(f"- {md}")
            content_parts.append("")
        if stock_summary.get("highlights"):
            for h in stock_summary["highlights"][:4]:
                content_parts.append(h)
        elif stock_summary.get("key_summary"):
            for h in stock_summary["key_summary"][:4]:
                content_parts.append(h)
        content_parts.append(f"\n[ìƒì„¸ ë³´ê¸°]({stock_summary.get('url', '#')})\n")

    # Regulatory section
    if regulatory_summary:
        content_parts.append(f"### ê·œì œ ë™í–¥ ({regulatory_summary['count']}ê±´)\n")
        if regulatory_summary.get("key_summary"):
            for h in regulatory_summary["key_summary"][:3]:
                content_parts.append(h)
        content_parts.append(f"\n[ìƒì„¸ ë³´ê¸°]({regulatory_summary.get('url', '#')})\n")

    # Security section
    if security_summary:
        content_parts.append(f"### ë³´ì•ˆ ë¦¬í¬íŠ¸ ({security_summary['count']}ê±´)\n")
        if security_summary.get("key_summary"):
            for h in security_summary["key_summary"][:3]:
                content_parts.append(h)
        if security_summary.get("incidents"):
            content_parts.append("\n| í”„ë¡œì íŠ¸ | í”¼í•´ ê·œëª¨ | ê³µê²© ìœ í˜• |")
            content_parts.append("|----------|----------|----------|")
            for row in security_summary["incidents"][:3]:
                content_parts.append(row)
        content_parts.append(f"\n[ìƒì„¸ ë³´ê¸°]({security_summary.get('url', '#')})\n")

    # Social section
    if social_summary:
        content_parts.append(f"### ì†Œì…œ ë¯¸ë””ì–´ ë™í–¥ ({social_summary['count']}ê±´)\n")
        if social_summary.get("highlights"):
            for h in social_summary["highlights"][:3]:
                content_parts.append(h)
        elif social_summary.get("key_summary"):
            for h in social_summary["key_summary"][:3]:
                content_parts.append(h)
        content_parts.append(f"\n[ìƒì„¸ ë³´ê¸°]({social_summary.get('url', '#')})\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 7. NOTABLE NEWS (P2)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if priority_items.get("P2"):
        content_parts.append("---\n")
        content_parts.append("## ì£¼ëª©í•  ì†Œì‹\n")
        seen_p2 = set()
        for item in priority_items["P2"][:5]:
            title = item.get("title", "")
            norm = re.sub(r"[^a-zê°€-í£0-9]", "", item.get("description", title).lower())
            if norm in seen_p2:
                continue
            seen_p2.add(norm)
            content_parts.append(f"- {title}")
        content_parts.append("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 8. REPORT LINKS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    content_parts.append("---\n")
    content_parts.append("## ìƒì„¸ ë¦¬í¬íŠ¸ ë§í¬\n")
    content_parts.append("| ë¦¬í¬íŠ¸ | ìˆ˜ì§‘ ê±´ìˆ˜ | ë§í¬ |")
    content_parts.append("|:---|:---:|:---|")
    for name, count, url in post_links:
        count_str = f"{count}ê±´" if count else "-"
        content_parts.append(f"| {name} | {count_str} | [ë°”ë¡œê°€ê¸°]({url}) |")

    content_parts.append("\n---\n")
    content_parts.append(
        "*ë³¸ ìš”ì•½ì€ ìë™ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤.*"
    )

    content = "\n".join(content_parts)

    # Create post with pin: true
    title = f"ì¼ì¼ ë‰´ìŠ¤ ì¢…í•© ìš”ì•½ - {today}"
    slug = "daily-news-summary"
    filename = f"{today}-{slug}.md"
    filepath = os.path.join(POSTS_DIR, filename)

    tags = ["ì¼ì¼ìš”ì•½", "ì•”í˜¸í™”í", "ì£¼ì‹", "ê·œì œ", "ì†Œì…œë¯¸ë””ì–´", "ë³´ì•ˆ", "ì •ì¹˜ì¸ê±°ë˜"]
    escaped_title = title.replace('"', '\\"')

    frontmatter = f"""---
title: "{escaped_title}"
date: {today} 12:00:00 +0900
categories: [market-analysis]
tags: [{", ".join(tags)}]
source: "consolidated"
lang: "ko"
pin: true
excerpt: "{counts_str}ì˜ ë‰´ìŠ¤ë¥¼ ì¢…í•© ë¶„ì„í•œ ì¼ì¼ ìš”ì•½"
---"""

    post_content = frontmatter + "\n\n" + content.strip()

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(post_content)

    logger.info(
        "Created daily summary: %s (total %d news items)", filepath, total_count
    )
    logger.info("=== Daily summary generation complete ===")


if __name__ == "__main__":
    main()
